 #Training
 batch_size: 256 #how many samples per batch
 epochs: 50 #how many epochs to train for (maximum epochs)
 early_stopping_patience: 0 #How many epochs to continue training with no improvement. -1 for infinite patience.
 loss: L2 # "L1", "L2" or "corr" for training using the corresponding loss function
 dropout: 0.5 #dropout probability
 weight_decay: 0.01 #L2 weight decay strength
 lr: 0.001 #L2 weight decay strength
 num_steps: 15 #how many steps to unfold the SNN for in training

#Neuron
 Vth: 1 #Threshold value for the LIF neurons

 beta: 0.96 #Initial value of the decay parameter in the LIF neurons
 beta_trainable: True #Trainable or non-trainable decay
 mem_trainable: True #Trainable or non-trainable membrane potential
 constrain_method: 'none' #When to constrain the decay parameters range.
 #options are 'none' (no constraining), 'forward' (constraining only in the forward pass),
 #'always' (always strictly constraining the value) and 'eval'(only constrain the value in the evaluation)

 surrogate_gradient: surrogate.fast_sigmoid(slope=20) #surrogate gradient function. Currently, only 'square' is implemented


#Dataset
 output_type: 'vel' #what to predict, options are 'pos', 'vel', 'acc'.

#Network
 batchnorm: 'none' #What Batch normalisation to use. Currently only 'tdBN' (threshold dependent batch normalisation) and 'none' implemented.
 layer1_neuron: 32 #count of neurons per fully connected layer
 layer2_neuron: 48 #count of neurons per fully connected layer
 use_bias: True #Are the Neurons are allowed to have a bias or not

#Reporting
 loss_steps: 20 #How often to report the loss during training in the report file (after every xth Batch)
 output_report: data.txt #Where to save the results and training info to
 output_plots: plots/ #which folder to save the plots to

#General
 seed: [123, 345, 456] #Seed for initialisation. Use 'random' for a non-deterministic seed
 device: cpu #Which cuda device to train on
 name: 'eth' #How to store files
 trials: 10 #How many initialisations to evaluate the network for
 5-fold: False #Use 5-fold validation or not:
 #True -> Split the training dataset into 5 folds, then use 4 for training and 1 for (holdout) evaluation. Repeat 5 times such that evey fold is the evaluation set once.
 #False -> Use the entire training dataset for training and test the network on the test set. DO NOT USE FOR HYPERPARRAMETER TUNING!
 early_stopping: True #Use early stopping or not
 dataset_file: /Users/zby/PycharmProjects/BMI/dataset/ #File where the preprocessed dataset is found
 postpr_data_path: /Users/zby/PycharmProjects/BMI/dataset/postpr_data/snn_0.016_s1/
 filename: indy_20160630_01_v7
 d_type: torch.float

#save and load model
 save_model: False #If the model should be saved
 save_model_dir: model/ #which folder to save the model to
 load_model: False  #If a model should be loaded and evaluated (without further training)
 load_model_dir: model/model_trial_0_fold_0_epoch_42_id_993906.pt #Which file to load the model from